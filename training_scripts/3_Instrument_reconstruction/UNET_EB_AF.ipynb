{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import fft\n",
    "import wave\n",
    "import sys\n",
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as signal\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/User/Desktop/AppStat/MachineLearning/AppliedML2024/final_project/data/nsynth-valid/audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_dir(directory):\n",
    "    filenames = os.listdir(directory)\n",
    "    return filenames\n",
    "filenames = read_files_in_dir(path)\n",
    "#pianos = [filename for filename in filenames if \"piano\" in filename] #empty\n",
    "bass = [filename for filename in filenames if \"bass\" in filename]\n",
    "guitar = [filename for filename in filenames if \"guitar\" in filename]\n",
    "#drum = [filename for filename in filenames if \"drum\" in filename] #empty\n",
    "flutes = [filename for filename in filenames if \"flute\" in filename]\n",
    "keyboards = [filename for filename in filenames if \"keyboard\" in filename] \n",
    "guitar_acoustic = [filename for filename in filenames if \"guitar_acoustic\" in filename]\n",
    "flute_acoustic = [filename for filename in filenames if \"flute_acoustic\" in filename]\n",
    "bass_electronic = [filename for filename in filenames if \"bass_electronic\" in filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def audio_to_waveform(audio):\n",
    "    waveform, sr = librosa.load(audio, sr=None)\n",
    "    return waveform, sr\n",
    "\n",
    "def waveform_to_spectogram(waveform):\n",
    "    if waveform.dtype != np.float32:\n",
    "        waveform = waveform.astype(np.float32)\n",
    "    spectrogram = librosa.stft(waveform)\n",
    "    return np.abs(spectrogram)\n",
    "\n",
    "def pad_spectrogram(spec, target_shape):\n",
    "    padded_spec = np.zeros(target_shape,dtype = np.float32)\n",
    "    min_shape = np.minimum(target_shape, spec.shape)\n",
    "    padded_spec[:min_shape[0], :min_shape[1]] = spec[:min_shape[0], :min_shape[1]]\n",
    "    return padded_spec\n",
    "\n",
    "def spectogram_to_audio(spectrogram, sr,output_wav):\n",
    "    waveform = librosa.istft(spectrogram)\n",
    "    waveform = np.nan_to_num(waveform)\n",
    "    waveform = waveform/np.max(np.abs(waveform))\n",
    "    return write(output_wav, sr, (waveform*32767).astype(np.int16))\n",
    "\n",
    "\n",
    "def normalize_spectrogram(spectrogram):\n",
    "    min_val = np.min(spectrogram)\n",
    "    max_val = np.max(spectrogram)\n",
    "    normalized_spectrogram = (spectrogram - min_val) / (max_val - min_val + 1e-6)\n",
    "    return normalized_spectrogram\n",
    "\n",
    "def pick_samples_and_classify(arrays):\n",
    "    #Picks a random number of samples, and returns their filepath and label\n",
    "    instruments = []\n",
    "    #pick at minimum two instruments\n",
    "    number_of_instruments = 2\n",
    "    #np.random.randint(2, len(arrays) + 1)\n",
    "    labels = np.zeros(len(arrays))\n",
    "    already_picked = []\n",
    "\n",
    "    while len(instruments) < number_of_instruments:\n",
    "        random_pick = np.random.randint(0, len(arrays))\n",
    "        if random_pick in already_picked:\n",
    "            break\n",
    "        else: \n",
    "            already_picked.append(random_pick)\n",
    "            pick = np.random.choice(arrays[random_pick], 1)\n",
    "            instruments.append(pick)\n",
    "            labels[random_pick] = 1\n",
    "\n",
    "    return instruments, labels\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#read the filenames, and add their data to 5 lists\n",
    "def add_waveform_to_list(filenames):\n",
    "    waveforms = []\n",
    "    for filename in filenames:\n",
    "        waveform, params = audio_to_waveform(path + filename[0])\n",
    "        waveforms.append(waveform)\n",
    "    return waveforms\n",
    "\n",
    "def find_longest_array(arrays):\n",
    "    longest = 0\n",
    "    for array in arrays:\n",
    "        if len(array) > longest:\n",
    "            longest = len(array)\n",
    "    return longest\n",
    "\n",
    "def combine_waveforms(waveforms):\n",
    "    normalization = 1 / len(waveforms)\n",
    "    # changed to be equal to the length of the longest waveform\n",
    "    out = np.zeros(find_longest_array(waveforms), dtype=np.float32)\n",
    "    for w in waveforms:\n",
    "        out += w.astype(np.float32) * normalization\n",
    "    return out # note, this retuns a float32 array - it is needed to convert this to int16 before saving it to a wav file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nu_gen_spectro(N, target_shape=(1025, 126)):\n",
    "    data = []\n",
    "    labels = []\n",
    "    original_labels = []\n",
    "    #instrument_list = [bass, guitar, flutes]\n",
    "    instrument_list = [bass_electronic, flute_acoustic]\n",
    "\n",
    "    for i in range(N):\n",
    "        paths, label = pick_samples_and_classify(instrument_list)\n",
    "        original_labels.append(label)\n",
    "        waveforms = add_waveform_to_list(paths)\n",
    "        mixed_waveform = combine_waveforms(waveforms)\n",
    "        \n",
    "        #audio = audio_to_waveform(mixed_waveform)\n",
    "        mixed_spectro = waveform_to_spectogram(mixed_waveform)\n",
    "        #mixed_spectro = audio_to_spectrogram(mixed_waveform)\n",
    "        mixed_spectro_padded = pad_spectrogram(mixed_spectro, target_shape)\n",
    "        mixed_spectro_normalized = normalize_spectrogram(mixed_spectro_padded)\n",
    "        \n",
    "        inter_waveforms = []\n",
    "        \n",
    "\n",
    "        inst_i = 0\n",
    "        for n, i in enumerate(label):\n",
    "            if i == 1:\n",
    "                spectro = waveform_to_spectogram(waveforms[inst_i])\n",
    "                spectro_padded = pad_spectrogram(spectro, target_shape)\n",
    "                inter_waveforms.append(spectro_padded)\n",
    "                \n",
    "                inst_i += 1\n",
    "\n",
    "            if i == 0:\n",
    "                inter_waveforms.append(np.zeros(target_shape))\n",
    "\n",
    "        data.append(mixed_spectro_normalized)\n",
    "        #inter_waveforms.append(np.zeros(target_shape))\n",
    "        labels.append(inter_waveforms)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    return data, np.array(labels), np.array(original_labels)# remove last line if you want to return only data and labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (1025, 126)\n",
    "N = 2000  # Number of samples\n",
    "\n",
    "\n",
    "data, labels, original_labels = nu_gen_spectro(N, target_shape)\n",
    "\n",
    "print(np.shape(data))  # Should be (N, 1025, 128)\n",
    "print(np.shape(labels))\n",
    "#print(np.shape(piano_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def reset_cuda_memory():\n",
    "    # Print memory usage before reset\n",
    "    print(\"Before reset:\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated()}\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved()}\")\n",
    "\n",
    "    # Reset PyTorch memory allocator\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Synchronize to ensure all operations are complete\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Print memory usage after reset\n",
    "    print(\"After reset:\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated()}\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved()}\")\n",
    "\n",
    "# Call the function to reset CUDA memory\n",
    "reset_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a multi-source audio seperation function using a unet model from torch\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=2):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Define the contracting/downsampling path\n",
    "        self.conv1 = nn.Conv2d(input_channels, 8, (3,125), padding=(1,62)) # (3,125), padding=(1,62).\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Add transpose convolutional layers for upsampling\n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.conv8 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv9 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv10 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.conv11 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.upconv5 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
    "        self.conv12 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        self.upconv6 = nn.ConvTranspose2d(16, 8, 2, stride=2)\n",
    "        self.conv13 = nn.Conv2d(16, 8, 3, padding=1)\n",
    "        self.final_conv = nn.Conv2d(8, output_channels, 3,padding = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting path\n",
    "        x = self.dropout(x)\n",
    "        x1 = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x2 = self.pool(nn.functional.relu(self.bn2(self.conv2(x1))))\n",
    "        x3 = self.pool(nn.functional.relu(self.bn3(self.conv3(x2))))\n",
    "        x4 = self.pool(nn.functional.relu(self.bn4(self.conv4(x3))))\n",
    "        x5 = self.pool(nn.functional.relu(self.bn5(self.conv5(x4))))\n",
    "        x6 = self.pool(nn.functional.relu(self.bn6(self.conv6(x5))))\n",
    "        x7 = self.pool(nn.functional.relu(self.bn7(self.conv7(x6))))\n",
    "        # Print sizes for debugging\n",
    "        #print(f\"x1 shape: {x1.shape}\")\n",
    "        #print(f\"x2 shape: {x2.shape}\")\n",
    "        #print(f\"x3 shape: {x3.shape}\")\n",
    "        #print(f\"x4 shape: {x4.shape}\")\n",
    "        #print(f\"x5 shape: {x5.shape}\")\n",
    "        #print(f\"x6 shape: {x6.shape}\")\n",
    "        #print(f\"x7 shape: {x7.shape}\")\n",
    "        x = self.upconv1(x7)\n",
    "        x = F.interpolate(x, size=(x6.size(2), x6.size(3)), mode='nearest')\n",
    "    \n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x6], dim=1)\n",
    "        #print(f\"x shape after concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv8(x))\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = F.interpolate(x, size=(x5.size(2), x5.size(3)), mode='nearest')\n",
    "   \n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        #print(f\"x shape after concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv9(x))\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = F.interpolate(x, size=(x4.size(2), x4.size(3)), mode='nearest')\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv10(x))\n",
    "        \n",
    "        x = self.upconv4(x)\n",
    "        x = F.interpolate(x, size=(x3.size(2), x3.size(3)), mode='nearest')\n",
    "        #print(f\"x2_interp shape: {x3.shape}\")\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "    \n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv11(x))\n",
    "\n",
    "        x = self.upconv5(x)\n",
    "        x = F.interpolate(x, size=(x2.size(2), x2.size(3)), mode='nearest')\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv12(x))\n",
    "\n",
    "\n",
    "        x = self.upconv6(x)\n",
    "        #print(f'upconv6 shape: {x.shape}')\n",
    "        x = F.interpolate(x, size=(x1.size(2), x1.size(3)), mode='nearest')\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = F.relu(self.conv13(x))\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "#data = data[..., np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(y_train.shape)\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "# Set all values in x_train and x_test to be 0 for values lower than e-2\n",
    "X_train[X_train < 1e-2] = 0\n",
    "X_test[X_test < 1e-2] = 0\n",
    "# Create the datasets and dataloaders\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=60, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=60, shuffle=False)\n",
    "#print(train_dataset[0][0].shape)\n",
    "\n",
    "# Get a batch of data\n",
    "inputs, targets = next(iter(train_loader))\n",
    "# Print the shape of the inputs\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_normalize(spectogram):\n",
    "    min_val = torch.min(spectogram)\n",
    "    max_val = torch.max(spectogram)\n",
    "    normalized_spectogram = (spectogram - min_val) / (max_val - min_val)\n",
    "    return normalized_spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true,X):\n",
    "        #print(y_pred)\n",
    "        l1_loss0 = F.mse_loss(y_pred[:,0,:], y_true[:,0,:])\n",
    "        l1_loss1 = F.mse_loss(y_pred[:,1,:], y_true[:,1,:])\n",
    "        #l1_loss2 = F.mse_loss(y_pred[:,2,:], y_true[:,2,:])\n",
    "        #y_pred = torch.clamp(y_pred, min=0)\n",
    "        sq_X = torch.squeeze(X)\n",
    "        summed_spectogram = torch.sum(y_pred, dim=1)\n",
    "        diff = torch.abs(torch_normalize(summed_spectogram) - torch_normalize(sq_X))\n",
    "        regulator = 1000\n",
    "        penalty = torch.mean(diff)\n",
    "        #print(penalty)\n",
    "        l1_loss = torch.mean(l1_loss0 + l1_loss1 ) + regulator*penalty\n",
    "        return l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SDRLoss(nn.Module):\n",
    "    def __init__(self, smoothness_lambda=0.1, deviation_lambda=0.1,diversity_lambda = 1):\n",
    "        super(SDRLoss, self).__init__()\n",
    "        self.smoothness_lambda = smoothness_lambda\n",
    "        self.deviation_lambda = deviation_lambda\n",
    "        self.diversity_lambda = diversity_lambda\n",
    "    def forward(self, y_pred, y_true, X, model_parameters=None):\n",
    "\n",
    "        #y_true = torch_normalize(y_true)\n",
    "        #y_pred = torch_normalize(y_pred)\n",
    "        delta = 1e-7  # avoid numerical errors\n",
    "        num = torch.sum(torch.square(torch.squeeze(y_true)), dim=(2, 3))\n",
    "        den = torch.sum(torch.square(torch.squeeze(y_true - y_pred)), dim=(2, 3))\n",
    "        num += delta\n",
    "        den += delta\n",
    "        scores = 10 * torch.log10(num / den)\n",
    "\n",
    "\n",
    "\n",
    "        sq_X = torch.squeeze(X)\n",
    "        summed_spectogram = torch.sum(y_pred, dim=1)\n",
    "        diff = torch.abs(summed_spectogram - sq_X)\n",
    "\n",
    "        diversity_penalty = 0\n",
    "        num_sources = y_pred.size(1)\n",
    "        for i in range(num_sources):\n",
    "            for j in range(i + 1, num_sources):\n",
    "                similarity = torch.norm(y_pred[:, i, :, :] - y_pred[:, j, :, :], p=2).mean()\n",
    "                diversity_penalty += 1 / (similarity + 1e-7)  # Adding a small constant to avoid division by zero\n",
    "        diversity_penalty *= self.diversity_lambda\n",
    "\n",
    "        penalty_positive  = 0\n",
    "        for i in range(y_true.size(0)):\n",
    "            for j in range(y_true.size(1)):\n",
    "                mask = torch.gt(y_true[i, j, :, :], 0.0)\n",
    "                num_nonzero = torch.sum(mask)\n",
    "                if num_nonzero != 0:\n",
    "                    penalty_positive += torch.sum(torch.square(y_pred[i, j, :, :][mask] - y_true[i, j, :, :][mask])) / num_nonzero\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate penalty based on the minimum negative SDR score\n",
    "        negative_scores = torch.clamp(scores, max=0)\n",
    "        penalty_negative = -torch.sum(negative_scores)\n",
    "\n",
    "\n",
    "        total_penalty = (\n",
    "            diversity_penalty +\n",
    "            torch.mean(diff) * 1 + 0.1 * penalty_positive +  0.1*penalty_negative) # +self.smoothness_lambda * pit_loss_value)   \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Loss : {-torch.mean(scores) + total_penalty}, SDR : {torch.mean(scores)}, Penalty : {total_penalty}\", end = '\\r')\n",
    "        return -torch.mean(scores) + total_penalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_channels = X_train.shape[1]  # Get the number of input channels from the data\n",
    "print(input_channels)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "model = UNet(input_channels=input_channels,output_channels=2).to(device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "#print(model)\n",
    "# Train the model\n",
    "\n",
    "num_epochs = 300\n",
    "best_loss = np.inf\n",
    "patience = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32)\n",
    "        optimizer.zero_grad() \n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y, X)\n",
    "        #print(f\"Shape of x : {X.shape},Shape of y {y.shape}\")\n",
    "        \n",
    "        # Scales the loss, calls backward(), and unscales the gradients\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # Unscales the gradients and updates the parameters\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item() * X.size(0) # Accumulate loss\n",
    "        #print(f\"Loss: {loss.item()}\")\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    #Implement early stopping\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_pred = model(X)\n",
    "                loss = criterion(y_pred, y, X)\n",
    "            \n",
    "            val_loss += 4*loss.item() * X.size(0)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience > 5:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "    \n",
    "\n",
    "    epoch_loss = running_loss\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "   \n",
    "torch.save(model.state_dict(), 'UNET_SourceSeperation_Simple.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'UNET_SourceSeperation_Simple.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a random waveform\n",
    "instrument_list = [bass_electronic, flute_acoustic]\n",
    "#Pick a random sample of each instrument\n",
    "filepaths, labels = pick_samples_and_classify(instrument_list)\n",
    "print(filepaths)\n",
    "#Extract .wav data into to a list using librosa\n",
    "waveforms = add_waveform_to_list(filepaths)\n",
    "\n",
    "#Combine the waveforms\n",
    "combined_waveform = combine_waveforms(waveforms)\n",
    "\n",
    "# Convert the combined waveform to a spectrogram\n",
    "spectrogram = waveform_to_spectogram(combined_waveform)\n",
    "\n",
    "# Set all values lower than e-2 to 0\n",
    "spectogram_0 = np.where(spectrogram < 1e-2, 0, spectrogram)\n",
    "# Normalize the spectrogram\n",
    "normalized_spectrogram = normalize_spectrogram(spectrogram)\n",
    "normalized_spectrogram_0 = normalize_spectrogram(spectogram_0)\n",
    "# Convert the normalized spectrogram back to audio and save it\n",
    "spectogram_to_audio(spectogram_0, 16000, \"combo1.wav\")\n",
    "spectogram_to_audio(spectrogram, 16000, \"combo2.wav\")\n",
    "\n",
    "# Function to plot spectrogram using librosa\n",
    "def plot_spectrogram(waveform, sr, title):\n",
    "    # Compute the STFT\n",
    "    D = librosa.stft(waveform)\n",
    "    # Convert to magnitude\n",
    "    D_mag = np.abs(D)\n",
    "    print(D_mag.shape)\n",
    "    # Plot the spectrogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(D_mag, ref=np.max), sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the spectrograms for the individual waveforms\n",
    "sr = 16000  # Assuming a sample rate of 16kHz\n",
    "\n",
    "plot_spectrogram(waveforms[0], sr, 'Spectrogram of First Waveform')\n",
    "plot_spectrogram(waveforms[1], sr, 'Spectrogram of Second Waveform')\n",
    "\n",
    "# If you want to plot the spectrogram of the combined waveform as well\n",
    "plot_spectrogram(combined_waveform, sr, 'Spectrogram of Combined Waveform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import wave\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "# Initialize and load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(input_channels=1,output_channels=2).to(device, dtype=torch.float32)\n",
    "model.load_state_dict(torch.load('UNET_SourceSeperation_Simple.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with wave.open('combo2.wav', 'rb') as wf:\n",
    "    input_audio = wf.readframes(-1)\n",
    "    input_sr = wf.getframerate()\n",
    "\n",
    "#Load the combined audio waveform\n",
    "combined_waveform, input_sr = librosa.load('combo2.wav', sr=None)\n",
    "\n",
    "# Compute spectrogram using scipy signal\n",
    "#f, t, Zxx = signal.spectrogram(combined_waveform, fs=input_sr, nperseg=256)\n",
    "spectrogram = waveform_to_spectogram(combined_waveform)\n",
    "#print(spectrogram.shape)\n",
    "# Convert spectrogram to PyTorch tensor\n",
    "input_tensor = torch.tensor(spectrogram, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device, dtype=torch.float32)  # Add batch and channel dimensions\n",
    "#print(input_tensor[0])\n",
    "# Pass input through the model to get predictions\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor)\n",
    "# Renormalize the output tensor\n",
    "def plot_separated_spectrograms(output_tensor, sr):\n",
    "    for i, spec in enumerate(output_tensor):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        librosa.display.specshow(librosa.amplitude_to_db(spec, ref=np.max), sr=sr, x_axis='time', y_axis='log')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.ylabel('Frequency [Hz]')\n",
    "        plt.xlabel('Time [sec]')\n",
    "        plt.title(f'Separated Spectrogram {i + 1}')\n",
    "        plt.show()\n",
    "        \n",
    "# Convert output tensor to numpy array and save as audio files\n",
    "output_tensor = output_tensor.squeeze().cpu().numpy()\n",
    "#print(output_tensor[0])\n",
    "# Plot the separated spectrograms\n",
    "plot_separated_spectrograms(output_tensor, input_sr)\n",
    "output_folder = \"output_audio\"\n",
    "#print(output_tensor)\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Convert each source back from spectrogram to audio and save\n",
    "for i, source in enumerate(output_tensor):\n",
    "    # Convert the spectrogram back to audio\n",
    "\n",
    "    # Save as .wav file\n",
    "    output_filepath = os.path.join(output_folder, f'source_{i}.wav')\n",
    "    spectogram_to_audio(source, input_sr, output_filepath)\n",
    "    print(f\"Saved {output_filepath}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
